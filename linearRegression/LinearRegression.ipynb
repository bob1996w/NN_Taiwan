{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "#import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from tensorflow.contrib.layers import fully_connected, batch_norm\n",
    "import math\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./DL_final_project/DL_Taiwan_data/sinica/201701_Taiwan.csv',\n",
    "                 usecols=['Date','Time','PM2.5','PM10','PM1','Temperature','Humidity','lon','lat'])\n",
    "#把201701_Taiwan.csv的header的' lat',' lon'改成'lat','lon' (多了空格)\n",
    "train_for_taiwan = False # filter out non-Taiwan\n",
    "test_for_taiwan = False # filter out non-Taiwan\n",
    "#len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if train_for_taiwan:\n",
    "    df = df[(df['lat']>= 21 )& (df['lat'] <= 27)&(df['lon']>=118)&(df['lon']<=122)]\n",
    "df = df.assign(Timestamp = pd.to_datetime(df['Date']+' '+df['Time']))\n",
    "df = df.assign(Hour = df['Timestamp'].dt.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_X = df[['Hour','PM10','PM1','Temperature','Humidity','lon','lat']]\n",
    "df_X = df_X.values\n",
    "df_Y = df[['PM2.5']]\n",
    "df_Y = df_Y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_logdir = \"tf_logs\"\n",
    "batch_log_step = 50\n",
    "early_stopping_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dnn(X_1, y_1, X_2, y_2, X_3, y_3):\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # check folder\n",
    "    export_check_num = 1\n",
    "    export_dir = './final_model/'\n",
    "    while os.path.exists(export_dir):\n",
    "        export_check_num += 1\n",
    "        export_dir = './final_model_'+str(export_check_num)+'/'\n",
    "    \n",
    "    print(\"model will be exported in\",export_dir)\n",
    "    \n",
    "    # logs\n",
    "    start_time = datetime.now()\n",
    "    now = start_time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    logdir = \"./{}/run-{}\".format(root_logdir, now)\n",
    "    print(\"now=\",now)\n",
    "    \n",
    "    # dnn graph defs\n",
    "    n_input = 7\n",
    "    n_epochs = 100\n",
    "    #n_hidden = [100,100,10]\n",
    "    act_fn = tf.nn.sigmoid\n",
    "    learning_rate = 0.001\n",
    "    #batch_normalization = False\n",
    "    batch_size = 10000\n",
    "    #batch_size = 1000 #小範圍測試用\n",
    "    mult_bias = 1\n",
    "\n",
    "    # I/O\n",
    "    with tf.name_scope(\"Input\"):\n",
    "        X = tf.placeholder(tf.float32, [None, n_input], name=\"X\")\n",
    "        is_training = tf.placeholder(tf.bool, shape=(), name=\"is_training\")\n",
    "    with tf.name_scope(\"Output\"):\n",
    "        y = tf.placeholder(tf.float32, [None, 1], name=\"y\")\n",
    "        y_biased = y/mult_bias\n",
    "    \n",
    "    # batch norm \n",
    "    \n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    '''\n",
    "    with tf.name_scope(\"BatchNormArgs\"):\n",
    "        bn_params = {\n",
    "            'is_training': is_training,\n",
    "            'decay': 0.99,\n",
    "            'updates_collections': None,\n",
    "            'scale': True\n",
    "        }\n",
    "    '''\n",
    "    \n",
    "    # DNN\n",
    "    with tf.name_scope(\"DNN\"):\n",
    "        '''\n",
    "        with tf.contrib.framework.arg_scope(\n",
    "                [fully_connected],\n",
    "                weights_initializer = he_init,\n",
    "                normalizer_fn = batch_norm if batch_normalization else None,\n",
    "                normalizer_params = bn_params if batch_normalization else None\n",
    "                ):\n",
    "            h1=fully_connected(X ,n_hidden[0],activation_fn=act_fn,scope=\"h1\")\n",
    "            h2=fully_connected(h1,n_hidden[1],activation_fn=act_fn,scope=\"h2\")\n",
    "            h3=fully_connected(h2,n_hidden[2],activation_fn=act_fn,scope=\"h3\")\n",
    "        '''\n",
    "        logits=fully_connected(X, 1, weights_initializer=he_init, scope=\"out\")\n",
    "    \n",
    "    with tf.name_scope(\"Cost\"):\n",
    "        #cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y), name=\"cost\")\n",
    "        cost = tf.losses.mean_squared_error(logits, y_biased)\n",
    "    with tf.name_scope(\"AdamOptimizer\"):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "        minimizer = optimizer.minimize(cost)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.name_scope(\"ModelSaver\"):\n",
    "        saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.name_scope(\"Predict\"):\n",
    "        predict = tf.multiply(logits, mult_bias, name=\"predict\")\n",
    "        \n",
    "    with tf.name_scope(\"Error\"):\n",
    "        ave_of_batch_y = tf.reduce_mean(y_biased)\n",
    "        error = tf.abs(logits - y_biased)/ave_of_batch_y\n",
    "        relative_err = tf.reduce_mean(error)\n",
    "    \n",
    "    with tf.name_scope(\"Summaries-Train\"):\n",
    "        cost_summary = tf.summary.scalar('cost_function',cost)\n",
    "        error_summary = tf.summary.scalar('relative_err',relative_err)\n",
    "    with tf.name_scope(\"Summaries-Validation\"):\n",
    "        v_cost_summary = tf.summary.scalar('v_cost_function',cost)\n",
    "        v_error_summary = tf.summary.scalar('v_relative_err',relative_err)\n",
    "    file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        \n",
    "        total_batch = len(X_1)//batch_size\n",
    "        #total_batch = validation_idx_start//batch_size\n",
    "        print(\"Total batch:\",total_batch)\n",
    "        #X_va, y_va = dfa_X[validation_idx_start:test_idx_start], dfa_Y[validation_idx_start:test_idx_start]\n",
    "        X_va = X_2\n",
    "        y_va = y_2\n",
    "        X_test = X_3\n",
    "        y_test = y_3\n",
    "        \n",
    "        best_va_err_triggered = False\n",
    "        best_va_err = 0\n",
    "        early_stopping_triggered = False\n",
    "        stopping_epoch = 0\n",
    "        step = 0\n",
    "        save_path = \"\"\n",
    "        best_save_path = \"\"\n",
    "        \n",
    "        perm = np.arange(len(X_1))\n",
    "        np.random.shuffle(perm)\n",
    "        X_t = X_1[perm]\n",
    "        y_t = y_1[perm]\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            for batch in range(total_batch):\n",
    "                X_ba = X_t[batch*batch_size: (batch+1)*batch_size]\n",
    "                y_ba = y_t[batch*batch_size: (batch+1)*batch_size]\n",
    "                _, c = sess.run([minimizer, cost], feed_dict = {is_training: True, X:X_ba, y:y_ba})\n",
    "                \n",
    "                if batch % batch_log_step == 0:\n",
    "                    print(\"Epoch %4d batch %5d\"%(epoch,batch))\n",
    "                    cost_summary_str = cost_summary.eval(feed_dict={is_training:False, X:X_ba, y:y_ba})\n",
    "                    step = epoch * total_batch + batch\n",
    "                    file_writer.add_summary(cost_summary_str, step)\n",
    "                    error_summary_str = error_summary.eval(feed_dict={is_training:False, X:X_ba, y:y_ba})\n",
    "                    file_writer.add_summary(error_summary_str, step)\n",
    "            \n",
    "            save_path = saver.save(sess, \"./checkpoint/model_\"+now+\".ckpt\")\n",
    "            va_err, va_cost = sess.run([relative_err, cost], feed_dict={is_training:False, X:X_va, y:y_va})\n",
    "            va_l = sess.run(predict, feed_dict={is_training:False, X:X_va, y:y_va})\n",
    "            rnd_result_idx = random.randint(0, len(X_va)-1)\n",
    "            \n",
    "            print(va_l[rnd_result_idx], y_va[rnd_result_idx]) # print 1st prediction result\n",
    "            v_error_summary_str = v_error_summary.eval(feed_dict={is_training:False, X:X_va, y:y_va})\n",
    "            file_writer.add_summary(v_error_summary_str, step)\n",
    "            v_cost_summary_str = v_cost_summary.eval(feed_dict={is_training:False, X:X_va, y:y_va})\n",
    "            file_writer.add_summary(v_cost_summary_str, step)\n",
    "            print(\"Epoch %4d val.cost %3.6f val.err %3.2f%%\"%(epoch,va_cost,va_err*100),end=\" \")\n",
    "            \n",
    "            if best_va_err_triggered:\n",
    "                if va_err < best_va_err:\n",
    "                    print(\"best\")\n",
    "                    stopping_epoch = 0\n",
    "                    best_va_err = va_err\n",
    "                    \n",
    "                    best_save_path = saver.save(sess, \"./best_model/model_\"+now)\n",
    "                else:\n",
    "                    stopping_epoch += 1\n",
    "                    print(\"stopping %3d\"%stopping_epoch)\n",
    "                if stopping_epoch >= early_stopping_epochs:\n",
    "                    early_stopping_triggered = True\n",
    "                    print(\"Early stopping triggered: Step: %10d, val.err %3.2f%%\"%(step, va_err*100))\n",
    "            else:\n",
    "                best_va_err = va_err\n",
    "                best_va_err_triggered = True\n",
    "                print(\"best\")\n",
    "                best_save_path = saver.save(sess, \"./best_model/model_\"+now)\n",
    "            \n",
    "            if early_stopping_triggered:\n",
    "                break\n",
    "        \n",
    "        finish_time = datetime.now()\n",
    "        print(\"best model saved to:\", best_save_path)\n",
    "        file_writer.close()\n",
    "        elapse_time = finish_time - start_time\n",
    "        total_seconds = elapse_time.total_seconds()\n",
    "        print(\"Total time:\", total_seconds)\n",
    "        \n",
    "        #X_test = dfa_X[test_idx_start:idx_end]\n",
    "        #y_test = dfa_Y[test_idx_start:idx_end]\n",
    "        saver.restore(sess, \"./best_model/model_\"+now)\n",
    "        best_err = relative_err.eval({is_training: False, X: X_test, y: y_test})\n",
    "        print(\"Test Err: %3.2f%%\"%(best_err*100))\n",
    "        \n",
    "        \n",
    "        builder = tf.saved_model.builder.SavedModelBuilder(export_dir)\n",
    "        builder.add_meta_graph_and_variables(sess, [\"tag\"], signature_def_map= {\n",
    "            \"model\": tf.saved_model.signature_def_utils.predict_signature_def(\n",
    "                inputs= {\"X\": X, \"is_training\": is_training},\n",
    "                outputs= {\"predict\": predict})\n",
    "        })\n",
    "        builder.save()\n",
    "        \n",
    "        \"\"\"\n",
    "        print(\"Predicting...\")\n",
    "        predict_values = sess.run(predict, feed_dict={is_training: False, X: X_w})\n",
    "        print(\"Predict End\")\n",
    "        return predict_values\n",
    "        \"\"\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df2: validation\n",
    "# df3: test\n",
    "df_v = pd.read_csv('./DL_final_project/DL_Taiwan_data/sinica/201702_Taiwan.csv',\n",
    "                 usecols=['Date','Time','PM2.5','PM10','PM1','Temperature','Humidity','lon','lat'])\n",
    "test_idx_start = math.floor(len(df_v)*0.5)\n",
    "df2 = df_v[:test_idx_start]\n",
    "df3 = df_v[test_idx_start:]\n",
    "if train_for_taiwan:\n",
    "    df2 = df2[(df2['lat']>= 21 )& (df2['lat'] <= 27)&(df2['lon']>=118)&(df2['lon']<=122)]\n",
    "if test_for_taiwan:\n",
    "    df3 = df3[(df3['lat']>= 21 )& (df3['lat'] <= 27)&(df3['lon']>=118)&(df3['lon']<=122)]\n",
    "df2 = df2.assign(Timestamp = pd.to_datetime(df2['Date']+' '+df2['Time']))\n",
    "df3 = df3.assign(Timestamp = pd.to_datetime(df3['Date']+' '+df3['Time']))\n",
    "df2 = df2.assign(Hour = df2['Timestamp'].dt.hour)\n",
    "df3 = df3.assign(Hour = df3['Timestamp'].dt.hour)\n",
    "df2_X = df2[['Hour','PM10','PM1','Temperature','Humidity','lon','lat']]\n",
    "df2_y = df2[['PM2.5']]\n",
    "df3_X = df3[['Hour','PM10','PM1','Temperature','Humidity','lon','lat']]\n",
    "df3_y = df3[['PM2.5']]\n",
    "df2_X = df2_X.values\n",
    "df2_y = df2_y.values\n",
    "df3_X = df3_X.values\n",
    "df3_y = df3_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model will be exported in ./final_model_4/\n",
      "now= 20180614_201119\n",
      "Total batch: 493\n",
      "Epoch    0 batch     0\n",
      "Epoch    0 batch    50\n",
      "Epoch    0 batch   100\n",
      "Epoch    0 batch   150\n",
      "Epoch    0 batch   200\n",
      "Epoch    0 batch   250\n",
      "Epoch    0 batch   300\n",
      "Epoch    0 batch   350\n",
      "Epoch    0 batch   400\n",
      "Epoch    0 batch   450\n",
      "[ 30.2564373] [48]\n",
      "Epoch    0 val.cost 797.667542 val.err 49.10% best\n",
      "Epoch    1 batch     0\n",
      "Epoch    1 batch    50\n",
      "Epoch    1 batch   100\n",
      "Epoch    1 batch   150\n",
      "Epoch    1 batch   200\n",
      "Epoch    1 batch   250\n",
      "Epoch    1 batch   300\n",
      "Epoch    1 batch   350\n",
      "Epoch    1 batch   400\n",
      "Epoch    1 batch   450\n",
      "[ 0.] [13]\n",
      "Epoch    1 val.cost 541.199951 val.err 37.04% best\n",
      "Epoch    2 batch     0\n",
      "Epoch    2 batch    50\n",
      "Epoch    2 batch   100\n",
      "Epoch    2 batch   150\n",
      "Epoch    2 batch   200\n",
      "Epoch    2 batch   250\n",
      "Epoch    2 batch   300\n",
      "Epoch    2 batch   350\n",
      "Epoch    2 batch   400\n",
      "Epoch    2 batch   450\n",
      "[ 0.] [46]\n",
      "Epoch    2 val.cost 424.883362 val.err 26.12% best\n",
      "Epoch    3 batch     0\n",
      "Epoch    3 batch    50\n",
      "Epoch    3 batch   100\n",
      "Epoch    3 batch   150\n",
      "Epoch    3 batch   200\n",
      "Epoch    3 batch   250\n",
      "Epoch    3 batch   300\n",
      "Epoch    3 batch   350\n",
      "Epoch    3 batch   400\n",
      "Epoch    3 batch   450\n",
      "[ 41.03303146] [34]\n",
      "Epoch    3 val.cost 280.467804 val.err 24.75% best\n",
      "Epoch    4 batch     0\n",
      "Epoch    4 batch    50\n",
      "Epoch    4 batch   100\n",
      "Epoch    4 batch   150\n",
      "Epoch    4 batch   200\n",
      "Epoch    4 batch   250\n",
      "Epoch    4 batch   300\n",
      "Epoch    4 batch   350\n",
      "Epoch    4 batch   400\n",
      "Epoch    4 batch   450\n",
      "[ 24.63991356] [13]\n",
      "Epoch    4 val.cost 250.986221 val.err 22.58% best\n",
      "Epoch    5 batch     0\n",
      "Epoch    5 batch    50\n",
      "Epoch    5 batch   100\n",
      "Epoch    5 batch   150\n",
      "Epoch    5 batch   200\n",
      "Epoch    5 batch   250\n",
      "Epoch    5 batch   300\n",
      "Epoch    5 batch   350\n",
      "Epoch    5 batch   400\n",
      "Epoch    5 batch   450\n",
      "[ 40.56563568] [45]\n",
      "Epoch    5 val.cost 251.345078 val.err 22.44% best\n",
      "Epoch    6 batch     0\n",
      "Epoch    6 batch    50\n",
      "Epoch    6 batch   100\n",
      "Epoch    6 batch   150\n",
      "Epoch    6 batch   200\n",
      "Epoch    6 batch   250\n",
      "Epoch    6 batch   300\n",
      "Epoch    6 batch   350\n",
      "Epoch    6 batch   400\n",
      "Epoch    6 batch   450\n",
      "[ 38.5375824] [36]\n",
      "Epoch    6 val.cost 251.855301 val.err 22.42% best\n",
      "Epoch    7 batch     0\n",
      "Epoch    7 batch    50\n",
      "Epoch    7 batch   100\n",
      "Epoch    7 batch   150\n",
      "Epoch    7 batch   200\n",
      "Epoch    7 batch   250\n",
      "Epoch    7 batch   300\n",
      "Epoch    7 batch   350\n",
      "Epoch    7 batch   400\n",
      "Epoch    7 batch   450\n",
      "[ 19.40884209] [47]\n",
      "Epoch    7 val.cost 252.186539 val.err 22.44% stopping   1\n",
      "Epoch    8 batch     0\n",
      "Epoch    8 batch    50\n",
      "Epoch    8 batch   100\n",
      "Epoch    8 batch   150\n",
      "Epoch    8 batch   200\n",
      "Epoch    8 batch   250\n",
      "Epoch    8 batch   300\n",
      "Epoch    8 batch   350\n",
      "Epoch    8 batch   400\n",
      "Epoch    8 batch   450\n",
      "[ 31.74224281] [25]\n",
      "Epoch    8 val.cost 252.338058 val.err 22.48% stopping   2\n",
      "Epoch    9 batch     0\n",
      "Epoch    9 batch    50\n",
      "Epoch    9 batch   100\n",
      "Epoch    9 batch   150\n",
      "Epoch    9 batch   200\n",
      "Epoch    9 batch   250\n",
      "Epoch    9 batch   300\n",
      "Epoch    9 batch   350\n",
      "Epoch    9 batch   400\n",
      "Epoch    9 batch   450\n",
      "[ 44.52087784] [38]\n",
      "Epoch    9 val.cost 252.407578 val.err 22.51% stopping   3\n",
      "Epoch   10 batch     0\n",
      "Epoch   10 batch    50\n",
      "Epoch   10 batch   100\n",
      "Epoch   10 batch   150\n",
      "Epoch   10 batch   200\n",
      "Epoch   10 batch   250\n",
      "Epoch   10 batch   300\n",
      "Epoch   10 batch   350\n",
      "Epoch   10 batch   400\n",
      "Epoch   10 batch   450\n",
      "[ 49.30102539] [50]\n",
      "Epoch   10 val.cost 252.484848 val.err 22.50% stopping   4\n",
      "Epoch   11 batch     0\n",
      "Epoch   11 batch    50\n",
      "Epoch   11 batch   100\n",
      "Epoch   11 batch   150\n",
      "Epoch   11 batch   200\n",
      "Epoch   11 batch   250\n",
      "Epoch   11 batch   300\n",
      "Epoch   11 batch   350\n",
      "Epoch   11 batch   400\n",
      "Epoch   11 batch   450\n",
      "[ 21.81078339] [7]\n",
      "Epoch   11 val.cost 252.586884 val.err 22.47% stopping   5\n",
      "Epoch   12 batch     0\n",
      "Epoch   12 batch    50\n",
      "Epoch   12 batch   100\n",
      "Epoch   12 batch   150\n",
      "Epoch   12 batch   200\n",
      "Epoch   12 batch   250\n",
      "Epoch   12 batch   300\n",
      "Epoch   12 batch   350\n",
      "Epoch   12 batch   400\n",
      "Epoch   12 batch   450\n",
      "[ 21.83960915] [28]\n",
      "Epoch   12 val.cost 252.683701 val.err 22.44% stopping   6\n",
      "Epoch   13 batch     0\n",
      "Epoch   13 batch    50\n",
      "Epoch   13 batch   100\n",
      "Epoch   13 batch   150\n",
      "Epoch   13 batch   200\n",
      "Epoch   13 batch   250\n",
      "Epoch   13 batch   300\n",
      "Epoch   13 batch   350\n",
      "Epoch   13 batch   400\n",
      "Epoch   13 batch   450\n",
      "[ 31.03004074] [23]\n",
      "Epoch   13 val.cost 252.775085 val.err 22.42% best\n",
      "Epoch   14 batch     0\n",
      "Epoch   14 batch    50\n",
      "Epoch   14 batch   100\n",
      "Epoch   14 batch   150\n",
      "Epoch   14 batch   200\n",
      "Epoch   14 batch   250\n",
      "Epoch   14 batch   300\n",
      "Epoch   14 batch   350\n",
      "Epoch   14 batch   400\n",
      "Epoch   14 batch   450\n",
      "[ 36.16059113] [29]\n",
      "Epoch   14 val.cost 252.853287 val.err 22.40% best\n",
      "Epoch   15 batch     0\n",
      "Epoch   15 batch    50\n",
      "Epoch   15 batch   100\n",
      "Epoch   15 batch   150\n",
      "Epoch   15 batch   200\n",
      "Epoch   15 batch   250\n",
      "Epoch   15 batch   300\n",
      "Epoch   15 batch   350\n",
      "Epoch   15 batch   400\n",
      "Epoch   15 batch   450\n",
      "[ 54.11368942] [52]\n",
      "Epoch   15 val.cost 252.913223 val.err 22.39% best\n",
      "Epoch   16 batch     0\n",
      "Epoch   16 batch    50\n",
      "Epoch   16 batch   100\n",
      "Epoch   16 batch   150\n",
      "Epoch   16 batch   200\n",
      "Epoch   16 batch   250\n",
      "Epoch   16 batch   300\n",
      "Epoch   16 batch   350\n",
      "Epoch   16 batch   400\n",
      "Epoch   16 batch   450\n",
      "[ 16.38497925] [9]\n",
      "Epoch   16 val.cost 252.958481 val.err 22.38% best\n",
      "Epoch   17 batch     0\n",
      "Epoch   17 batch    50\n",
      "Epoch   17 batch   100\n",
      "Epoch   17 batch   150\n",
      "Epoch   17 batch   200\n",
      "Epoch   17 batch   250\n",
      "Epoch   17 batch   300\n",
      "Epoch   17 batch   350\n",
      "Epoch   17 batch   400\n",
      "Epoch   17 batch   450\n",
      "[ 25.96531105] [15]\n",
      "Epoch   17 val.cost 252.993622 val.err 22.38% best\n",
      "Epoch   18 batch     0\n",
      "Epoch   18 batch    50\n",
      "Epoch   18 batch   100\n",
      "Epoch   18 batch   150\n",
      "Epoch   18 batch   200\n",
      "Epoch   18 batch   250\n",
      "Epoch   18 batch   300\n",
      "Epoch   18 batch   350\n",
      "Epoch   18 batch   400\n",
      "Epoch   18 batch   450\n",
      "[ 54.72754288] [57]\n",
      "Epoch   18 val.cost 253.022964 val.err 22.38% best\n",
      "Epoch   19 batch     0\n",
      "Epoch   19 batch    50\n",
      "Epoch   19 batch   100\n",
      "Epoch   19 batch   150\n",
      "Epoch   19 batch   200\n",
      "Epoch   19 batch   250\n",
      "Epoch   19 batch   300\n",
      "Epoch   19 batch   350\n",
      "Epoch   19 batch   400\n",
      "Epoch   19 batch   450\n",
      "[ 40.12096786] [28]\n",
      "Epoch   19 val.cost 253.048401 val.err 22.38% best\n",
      "Epoch   20 batch     0\n",
      "Epoch   20 batch    50\n",
      "Epoch   20 batch   100\n",
      "Epoch   20 batch   150\n",
      "Epoch   20 batch   200\n",
      "Epoch   20 batch   250\n",
      "Epoch   20 batch   300\n",
      "Epoch   20 batch   350\n",
      "Epoch   20 batch   400\n",
      "Epoch   20 batch   450\n",
      "[ 19.21811104] [29]\n",
      "Epoch   20 val.cost 253.071152 val.err 22.38% stopping   1\n",
      "Epoch   21 batch     0\n",
      "Epoch   21 batch    50\n",
      "Epoch   21 batch   100\n",
      "Epoch   21 batch   150\n",
      "Epoch   21 batch   200\n",
      "Epoch   21 batch   250\n",
      "Epoch   21 batch   300\n",
      "Epoch   21 batch   350\n",
      "Epoch   21 batch   400\n",
      "Epoch   21 batch   450\n",
      "[ 48.33470917] [44]\n",
      "Epoch   21 val.cost 253.092575 val.err 22.38% stopping   2\n",
      "Epoch   22 batch     0\n",
      "Epoch   22 batch    50\n",
      "Epoch   22 batch   100\n",
      "Epoch   22 batch   150\n",
      "Epoch   22 batch   200\n",
      "Epoch   22 batch   250\n",
      "Epoch   22 batch   300\n",
      "Epoch   22 batch   350\n",
      "Epoch   22 batch   400\n",
      "Epoch   22 batch   450\n",
      "[ 20.9073925] [53]\n",
      "Epoch   22 val.cost 253.112625 val.err 22.38% stopping   3\n",
      "Epoch   23 batch     0\n",
      "Epoch   23 batch    50\n",
      "Epoch   23 batch   100\n",
      "Epoch   23 batch   150\n",
      "Epoch   23 batch   200\n",
      "Epoch   23 batch   250\n",
      "Epoch   23 batch   300\n",
      "Epoch   23 batch   350\n",
      "Epoch   23 batch   400\n",
      "Epoch   23 batch   450\n",
      "[ 17.64537048] [52]\n",
      "Epoch   23 val.cost 253.132187 val.err 22.38% stopping   4\n",
      "Epoch   24 batch     0\n",
      "Epoch   24 batch    50\n",
      "Epoch   24 batch   100\n",
      "Epoch   24 batch   150\n",
      "Epoch   24 batch   200\n",
      "Epoch   24 batch   250\n",
      "Epoch   24 batch   300\n",
      "Epoch   24 batch   350\n",
      "Epoch   24 batch   400\n",
      "Epoch   24 batch   450\n",
      "[ 36.6007309] [27]\n",
      "Epoch   24 val.cost 253.151413 val.err 22.38% stopping   5\n",
      "Epoch   25 batch     0\n",
      "Epoch   25 batch    50\n",
      "Epoch   25 batch   100\n",
      "Epoch   25 batch   150\n",
      "Epoch   25 batch   200\n",
      "Epoch   25 batch   250\n",
      "Epoch   25 batch   300\n",
      "Epoch   25 batch   350\n",
      "Epoch   25 batch   400\n",
      "Epoch   25 batch   450\n",
      "[ 46.92996979] [44]\n",
      "Epoch   25 val.cost 253.170685 val.err 22.38% stopping   6\n",
      "Epoch   26 batch     0\n",
      "Epoch   26 batch    50\n",
      "Epoch   26 batch   100\n",
      "Epoch   26 batch   150\n",
      "Epoch   26 batch   200\n",
      "Epoch   26 batch   250\n",
      "Epoch   26 batch   300\n",
      "Epoch   26 batch   350\n",
      "Epoch   26 batch   400\n",
      "Epoch   26 batch   450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 54.9072876] [55]\n",
      "Epoch   26 val.cost 253.189545 val.err 22.38% stopping   7\n",
      "Epoch   27 batch     0\n",
      "Epoch   27 batch    50\n",
      "Epoch   27 batch   100\n",
      "Epoch   27 batch   150\n",
      "Epoch   27 batch   200\n",
      "Epoch   27 batch   250\n",
      "Epoch   27 batch   300\n",
      "Epoch   27 batch   350\n",
      "Epoch   27 batch   400\n",
      "Epoch   27 batch   450\n",
      "[ 55.64242935] [52]\n",
      "Epoch   27 val.cost 253.208038 val.err 22.38% stopping   8\n",
      "Epoch   28 batch     0\n",
      "Epoch   28 batch    50\n",
      "Epoch   28 batch   100\n",
      "Epoch   28 batch   150\n",
      "Epoch   28 batch   200\n",
      "Epoch   28 batch   250\n",
      "Epoch   28 batch   300\n",
      "Epoch   28 batch   350\n",
      "Epoch   28 batch   400\n",
      "Epoch   28 batch   450\n",
      "[ 51.13647461] [49]\n",
      "Epoch   28 val.cost 253.226685 val.err 22.38% stopping   9\n",
      "Epoch   29 batch     0\n",
      "Epoch   29 batch    50\n",
      "Epoch   29 batch   100\n",
      "Epoch   29 batch   150\n",
      "Epoch   29 batch   200\n",
      "Epoch   29 batch   250\n",
      "Epoch   29 batch   300\n",
      "Epoch   29 batch   350\n",
      "Epoch   29 batch   400\n",
      "Epoch   29 batch   450\n",
      "[ 48.49578857] [50]\n",
      "Epoch   29 val.cost 253.245056 val.err 22.38% stopping  10\n",
      "Early stopping triggered: Step:      14747, val.err 22.38%\n",
      "best model saved to: ./best_model/model_20180614_201119\n",
      "Total time: 69.325345\n",
      "INFO:tensorflow:Restoring parameters from ./best_model/model_20180614_201119\n",
      "Test Err: 22.88%\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: b'./final_model_4/saved_model.pb'\n"
     ]
    }
   ],
   "source": [
    "dnn(df_X, df_Y, df2_X, df2_y, df3_X, df3_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
